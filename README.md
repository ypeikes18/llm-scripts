# possible improvements
   # increase number of search results
   # use better embedding model
   # better llm model
   # Play with chunk size
   # better vector store that allows metadata search
   # use a sliding window to give chunks overlapping context

# DATA from parsing all but 6 episodes
    # ~32 million letters => 6 million words => 8 million tokens
    # 17 chunks > 4k letters
    # largest chunk 17993 letters -> 4k words -> 5500 tokens
    # largest episode 345575 letters -> 80k words -> 110000 tokens
    # When chunked by paragraph largest chunk 957 letters


